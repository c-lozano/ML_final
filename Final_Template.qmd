---
title: "Final Project for BIOS 26211"
author: "Your Name"
format: 
  html:
    self-contained: true
editor: visual
---

```{r setup}
#| include: false
library(tidyverse)
library(haven)
library(dplyr)
library(stringr)
library(sjlabelled)
library(tidyr)
library(reticulate)
# add any other libraries you need here
```

## Background

Summarize the system and the problem you are studying. Provide references that you learned from. Explain the main questions.

indicate who wrote the part

## The Data

Describe the data set you are using, including how it was measured. Describe all the variables and the challenges in their collection, e.g. were any observations not available or unreliable. Load the data set in the chunk(s) below and print out a few rows (e.g. using `glimpse`).

```{r}
Nutrition_D1 <- read_xpt('https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DR1TOT_L.xpt') 
Nutrition_D2 <- read_xpt('https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DR2TOT_L.xpt')
Medical_Conditions <- read_xpt('https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/MCQ_L.xpt')

head(Nutrition_D1)
head(Medical_Conditions)
```

Include your pipeline for cleaning and processing the data below, including removal of NAs or outliers, selecting variables, or calculating new ones:

```{r}
#Keeping SEQN and columns Energy - Caffeine
Filtered_Nutrition_D1 <- Nutrition_D1 %>% select(SEQN, 33:75) #Manually Selected Indeces
Filtered_Nutrition_D2 <- Nutrition_D2 %>% select(SEQN, 16:58) #Manually selected indices

#Renaming the columns (the naming comventions are confusing)
colnames(Filtered_Nutrition_D1)[-1] <- get_label(Filtered_Nutrition_D1)[-1]
colnames(Filtered_Nutrition_D2)[-1] <- get_label(Filtered_Nutrition_D2)[-1]

#Removing brackets to make reading column names easier
colnames(Filtered_Nutrition_D1) <- gsub("\\s*\\(.*?\\)", "", colnames(Filtered_Nutrition_D1))
colnames(Filtered_Nutrition_D2) <- gsub("\\s*\\(.*?\\)", "", colnames(Filtered_Nutrition_D2))

#Combining the datasets from the first day and second day by averaging them
Filtered_Nutrition_Avg <- Filtered_Nutrition_D1 %>%
  mutate(across(2:ncol(.), ~ (Filtered_Nutrition_D1[[cur_column()]] + Filtered_Nutrition_D2[[cur_column()]]) / 2))

#Removing NAs
Filtered_Nutrition_Avg_Cleaned <- na.omit(Filtered_Nutrition_Avg)
na_rows_removed <- nrow(Filtered_Nutrition_Avg) - nrow(Filtered_Nutrition_Avg_Cleaned)

#Manually Selecting the columns we want from the Questionnaire Data 
Filtered_Medical_Conditions <- Medical_Conditions %>%
  select(
    SEQN, MCQ010, AGQ030, MCQ160A, MCQ160B, MCQ160C, MCQ160E, MCQ160M, MCQ220
  ) %>%
  rename(
    Asthma = MCQ010,
    Hay_Fever = AGQ030,
    Arthritis = MCQ160A,
    Congestive_Heart_Failure = MCQ160B,
    Coronary_Heart_Disease = MCQ160C,
    Heart_Attack = MCQ160E,
    Thyroid_Problems = MCQ160M,
    Cancer = MCQ220
  )
Filtered_Medical_Conditions[is.na(Filtered_Medical_Conditions)] <- 0 # We will filter these out in our own codes, for now we keep them as 0s

# Merge both datasets by SEQN
Merged_Data <- inner_join(Filtered_Nutrition_Avg_Cleaned, Filtered_Medical_Conditions, by = "SEQN")

head(Merged_Data)
```

indicate who did the work: Alissa

## Methods

Describe the questions you posed and the methods used to address them. State the assumptions underlying the methods (e.g. explanatory variable is categorical, and predictors are independent) and demonstrate or discuss whether they are satisfied by your data. Provide at least one summary table or visualization of the relevant variables of your data.

### Method 1

indicate who did the work

```{r}

```

### Methods 2

indicate who did the work

```{r}

```

### Methods 3

I ran my code in python, since I have worked with their Neural Network packages before and I felt more comfortable using those. However, I can't find a way to run the code properly in RStudio, so I think if you wanted to run this code, you might need to run it on a python interpreter instead. Either way, I included the plots I got in my results section.

Alissa

```{python}
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve, auc
import numpy as np

# LOAD DATA
path = "/Users/alissadomenig/repositories/ML_final/Merged_Data.csv"
data = pd.read_csv(path)
X = data.iloc[:, :44].values
Y = data.iloc[:, 44:].values
scaler = StandardScaler()
X = scaler.fit_transform(X)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# BUILDING THE NEURAL NETWORK 
class SimpleNN(nn.Module):
    def __init__(self, input_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(0.1) # Using Dropout Layer to prevet overfitting
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)  
        x = self.relu(self.fc2(x))
        x = self.dropout(x)  
        x = self.fc3(x)
        x = self.sigmoid(x)
        return x

response_variable_names = data.columns[44:]

# SETTING UP CROSS VALIDATION
kf = KFold(n_splits=10, shuffle=True, random_state=42)

epochs = 50
patience = 5  

average_train_acc = {}
average_test_acc = {}
train_conf_matrices = {}
test_conf_matrices = {}
auc_scores = {}
roc_curves = {}

for i, response_name in enumerate(response_variable_names):
    y = Y[:, i].reshape(-1, 1)
    
    # CLEANING THE DATA SO WE ONLY KEEP YES = 1 AND NO = 0
    valid_indices = ((y == 1) | (y == 2)).flatten()
    X_valid = X[valid_indices]
    y_valid = y[valid_indices]
    y_valid = (y_valid == 1).astype(int) 

    fold_auc_scores = []
    fold_fprs = []
    fold_tprs = []
    train_acc_scores = []
    test_acc_scores = []
    train_conf_matrices[response_name] = []
    test_conf_matrices[response_name] = []

    ### TRAINING THE MODEL
    for fold, (train_idx, test_idx) in enumerate(kf.split(X_valid)):
        X_train, X_test = X_valid[train_idx], X_valid[test_idx]
        y_train, y_test = y_valid[train_idx], y_valid[test_idx]
        
        ##class_1_indices = np.where(y_train == 1)[0]
        ##class_0_indices = np.where(y_train == 0)[0]

        ##if len(class_1_indices) < len(class_0_indices):
        ##   sampled_class_0_indices = np.random.choice(class_0_indices, size=len     (class_1_indices), replace=False)
        ##else:
        ##    sampled_class_0_indices = class_0_indices

        ##balanced_train_indices = np.concatenate([class_1_indices, sampled_class_0_indices])
        ##X_train = X_train[balanced_train_indices]
        ##y_train = y_train[balanced_train_indices]

        # CONVERTING TO TENSORS
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)
        
        # CREATING DATASET AND DATA LOADER
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        # INITIALIZING MODEL
        model = SimpleNN(input_size=X.shape[1]).to(device)
        criterion = nn.BCELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.0001)
        best_val_loss = float('inf')
        patience_counter = 0
        best_model_state = None
        
        # TRAINING LOOP
        for epoch in range(epochs):
            model.train()
            batch_losses = []
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                predictions = model(batch_X).squeeze()
                loss = criterion(predictions, batch_y.squeeze())
                loss.backward()
                optimizer.step()
                batch_losses.append(loss.item())

            model.eval()
            with torch.no_grad():
                val_predictions = model(X_test_tensor).squeeze()
                val_loss = criterion(val_predictions, y_test_tensor.squeeze()).item()
            
            # USING EARLY STOPPING TO PREVENT OVERFITTING
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = model.state_dict()
            else:
                patience_counter += 1

            if patience_counter >= patience:
                print(f"Early stopping triggered for {response_name} at epoch {epoch+1}")
                break

        if best_model_state:
            model.load_state_dict(best_model_state)

        model.eval()
        with torch.no_grad():
            train_preds = (model(X_train_tensor).cpu().numpy().flatten() > 0.5).astype(int)
            test_preds = (model(X_test_tensor).cpu().numpy().flatten() > 0.5).astype(int)
        
        train_acc = accuracy_score(y_train, train_preds)
        test_acc = accuracy_score(y_test, test_preds)
        
        train_acc_scores.append(train_acc)
        test_acc_scores.append(test_acc)
        
        # COMPUTE CONFUSION MATRICES
        train_conf_matrix = confusion_matrix(y_train, train_preds)
        test_conf_matrix = confusion_matrix(y_test, test_preds)
        
        train_conf_matrices[response_name].append(train_conf_matrix)
        test_conf_matrices[response_name].append(test_conf_matrix)

        # COMPUTE ROC AND AUC CURVES
        with torch.no_grad():
            y_pred_probs = model(X_test_tensor).cpu().numpy().flatten()
        fpr, tpr, _ = roc_curve(y_test, y_pred_probs)
        roc_auc = auc(fpr, tpr)
        
        fold_auc_scores.append(roc_auc)
        fold_fprs.append(fpr)
        fold_tprs.append(tpr)
    
    # STORING AVERAGE TEST AND TRAIN ACCURACIES
    average_train_acc[response_name] = np.mean(train_acc_scores)
    average_test_acc[response_name] = np.mean(test_acc_scores)
    auc_scores[response_name] = fold_auc_scores
    roc_curves[response_name] = (fold_fprs, fold_tprs)
```

## Results

Produce figures or tables using the methods above to answer the questions you posed. For example, for a predictive model, provide cross-validation metrics and a plot of the predictions vs true values.

### Results 1

indicate who did the work

```{r}

```

### Results 2

indicate who did the work

```{r}

```

### Results 3

Alissa

```{python}
# VERTICAL WHISKER PLOT FOR AUC SCORES
plt.figure(figsize=(12, 6))
sns.boxplot(data=pd.DataFrame(auc_scores), orient="v", palette="Set2")
plt.xticks(rotation=20, fontsize=8)
plt.ylabel("AUC Score")
plt.title("AUC Scores for Each Response Variable")
plt.show()

# ROC CURVES FOR EACH RESPONSE VARIABLE
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
fig.suptitle("ROC Curves for Each Response Variable", fontsize=14)

for ax, (response_name, (fprs, tprs)) in zip(axes.flatten(), roc_curves.items()):
    for fold in range(len(fprs)):
        ax.plot(fprs[fold], tprs[fold], lw=2, alpha=0.5, label=f"Fold {fold+1}")
    ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Random Guess")
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.set_title(response_name)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# CONFUSION MATRICES FOR ALL RESPONSE VARIABLES
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
fig.suptitle("Confusion Matrices for Each Response Variable", fontsize=14)

for ax, (response_name, conf_matrices) in zip(axes.flatten(), train_conf_matrices.items()):
    avg_conf_matrix = np.mean(conf_matrices, axis=0)
    avg_conf_matrix = np.round(avg_conf_matrix).astype(int)
    sns.heatmap(avg_conf_matrix, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"],
                ax=ax)
    ax.set_title(response_name)
    ax.set_xlabel("Predicted")
    ax.set_ylabel("True")

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# PREDICTION ACCURACY
plt.figure(figsize=(12, 6))
names = list(average_train_acc.keys())
train_acc_values = [average_train_acc[name] for name in names]
test_acc_values = [average_test_acc[name] for name in names]
x = range(len(names))
width = 0.4
plt.bar(x, train_acc_values, width=width, label='Train Accuracy', color='green', alpha=0.6)
plt.bar([p + width for p in x], test_acc_values, width=width, label='Test Accuracy', color='red', alpha=0.6)
plt.xticks([p + width/2 for p in x], names, rotation=20, fontsize=8)
plt.ylabel("Accuracy")
plt.title("Average Prediction Accuracy across 10-Fold CV for Each Response Variable")
plt.legend()
plt.yticks(fontsize=10, rotation=45)
plt.show()
```

**Neural Network Plots**

![](images/ConfusionMatrices-01.png)

![](images/AUC-01.png)

![](images/AUC_curves-01.png)

![](images/Accuracy-01.png)

**Neural Network with Undersampling Plots**

![](images/ConfusionMatrices.png)![](images/AUC.png)![](images/AUC_curves.png)![](images/Accuracy.png)

## Conclusions

indicate who wrote it

Add a few paragraphs to describe what you learned and what new questions have been raised by your results.

## References

Links are best, but any format is fine as long as the reference is identifiable.
