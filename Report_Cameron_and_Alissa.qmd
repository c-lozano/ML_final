---
title: "Final Project for BIOS 26211"
author: "Your Name"
format: 
  html:
    self-contained: true
editor: source
---

```{r setup}
#| include: false
library(tidyverse)
library(haven)
library(dplyr)
library(stringr)
library(sjlabelled)
library(tidyr)
library(reticulate)
# add any other libraries you need here
```

## Background

Summarize the system and the problem you are studying. Provide references that you learned from. Explain the main questions.

indicate who wrote the part

## The Data

Describe the data set you are using, including how it was measured. Describe all the variables and the challenges in their collection, e.g. were any observations not available or unreliable. Load the data set in the chunk(s) below and print out a few rows (e.g. using `glimpse`).

```{r}
Nutrition_D1 <- read_xpt('https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DR1TOT_L.xpt') 
Nutrition_D2 <- read_xpt('https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DR2TOT_L.xpt')
Medical_Conditions <- read_xpt('https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/MCQ_L.xpt')

head(Nutrition_D1)
head(Medical_Conditions)
```

Include your pipeline for cleaning and processing the data below, including removal of NAs or outliers, selecting variables, or calculating new ones:

```{r}
#| eval: false
#| file: 'DataCleaning.R'
```

```{r}
#| eval: false
#| file: 'DataSplit.R'
```

indicate who did the work: Alissa and Cameron

## Methods

Describe the questions you posed and the methods used to address them. State the assumptions underlying the methods (e.g. explanatory variable is categorical, and predictors are independent) and demonstrate or discuss whether they are satisfied by your data. Provide at least one summary table or visualization of the relevant variables of your data.

### Bayes classifiers – Cameron

Three Bayes classification methods – naive Bayes, linear discriminant analysis (LDA), and quadratic discriminant analysis (QDA) were used to train a model to predict the diagnosis outcomes ("Yes/Diagnosed" or "No/Not Diagnosed") for each condition (separately) from the set of 43 nutrient predictor variables. Prediction accuracies and receiver operator characterisitic (ROC) curves were extracted from each model trained under 10-fold cross-validation to assess performance.

The code for training these models is reproduced below:
```{r}
#| eval: false
#| file: 'bayes_classifiers.R'
#| echo: !expr 1:206
```

### Trees and random forest – Romy

indicate who did the work

```{r}

```

### Neural network – Alissa

I ran my code in python, since I have worked with their Neural Network packages before and I felt more comfortable using those. However, I can't find a way to run the code properly in RStudio, so I think if you wanted to run this code, you might need to run it on a python interpreter instead. Either way, I included the plots I got in my results section.


```{python}
#| eval: false

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve, auc
import numpy as np

# LOAD DATA
path = "/Users/alissadomenig/repositories/ML_final/Merged_Data.csv"
data = pd.read_csv(path)
X = data.iloc[:, :44].values
Y = data.iloc[:, 44:].values
scaler = StandardScaler()
X = scaler.fit_transform(X)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# BUILDING THE NEURAL NETWORK 
class SimpleNN(nn.Module):
    def __init__(self, input_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(0.1) # Using Dropout Layer to prevet overfitting
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)  
        x = self.relu(self.fc2(x))
        x = self.dropout(x)  
        x = self.fc3(x)
        x = self.sigmoid(x)
        return x

response_variable_names = data.columns[44:]

# SETTING UP CROSS VALIDATION
kf = KFold(n_splits=10, shuffle=True, random_state=42)

epochs = 50
patience = 5  

average_train_acc = {}
average_test_acc = {}
train_conf_matrices = {}
test_conf_matrices = {}
auc_scores = {}
roc_curves = {}

for i, response_name in enumerate(response_variable_names):
    y = Y[:, i].reshape(-1, 1)
    
    # CLEANING THE DATA SO WE ONLY KEEP YES = 1 AND NO = 0
    valid_indices = ((y == 1) | (y == 2)).flatten()
    X_valid = X[valid_indices]
    y_valid = y[valid_indices]
    y_valid = (y_valid == 1).astype(int) 

    fold_auc_scores = []
    fold_fprs = []
    fold_tprs = []
    train_acc_scores = []
    test_acc_scores = []
    train_conf_matrices[response_name] = []
    test_conf_matrices[response_name] = []

    ### TRAINING THE MODEL
    for fold, (train_idx, test_idx) in enumerate(kf.split(X_valid)):
        X_train, X_test = X_valid[train_idx], X_valid[test_idx]
        y_train, y_test = y_valid[train_idx], y_valid[test_idx]
        
        ##class_1_indices = np.where(y_train == 1)[0]
        ##class_0_indices = np.where(y_train == 0)[0]

        ##if len(class_1_indices) < len(class_0_indices):
        ##   sampled_class_0_indices = np.random.choice(class_0_indices, size=len     (class_1_indices), replace=False)
        ##else:
        ##    sampled_class_0_indices = class_0_indices

        ##balanced_train_indices = np.concatenate([class_1_indices, sampled_class_0_indices])
        ##X_train = X_train[balanced_train_indices]
        ##y_train = y_train[balanced_train_indices]

        # CONVERTING TO TENSORS
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)
        
        # CREATING DATASET AND DATA LOADER
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        # INITIALIZING MODEL
        model = SimpleNN(input_size=X.shape[1]).to(device)
        criterion = nn.BCELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.0001)
        best_val_loss = float('inf')
        patience_counter = 0
        best_model_state = None
        
        # TRAINING LOOP
        for epoch in range(epochs):
            model.train()
            batch_losses = []
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                predictions = model(batch_X).squeeze()
                loss = criterion(predictions, batch_y.squeeze())
                loss.backward()
                optimizer.step()
                batch_losses.append(loss.item())

            model.eval()
            with torch.no_grad():
                val_predictions = model(X_test_tensor).squeeze()
                val_loss = criterion(val_predictions, y_test_tensor.squeeze()).item()
            
            # USING EARLY STOPPING TO PREVENT OVERFITTING
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = model.state_dict()
            else:
                patience_counter += 1

            if patience_counter >= patience:
                print(f"Early stopping triggered for {response_name} at epoch {epoch+1}")
                break

        if best_model_state:
            model.load_state_dict(best_model_state)

        model.eval()
        with torch.no_grad():
            train_preds = (model(X_train_tensor).cpu().numpy().flatten() > 0.5).astype(int)
            test_preds = (model(X_test_tensor).cpu().numpy().flatten() > 0.5).astype(int)
        
        train_acc = accuracy_score(y_train, train_preds)
        test_acc = accuracy_score(y_test, test_preds)
        
        train_acc_scores.append(train_acc)
        test_acc_scores.append(test_acc)
        
        # COMPUTE CONFUSION MATRICES
        train_conf_matrix = confusion_matrix(y_train, train_preds)
        test_conf_matrix = confusion_matrix(y_test, test_preds)
        
        train_conf_matrices[response_name].append(train_conf_matrix)
        test_conf_matrices[response_name].append(test_conf_matrix)

        # COMPUTE ROC AND AUC CURVES
        with torch.no_grad():
            y_pred_probs = model(X_test_tensor).cpu().numpy().flatten()
        fpr, tpr, _ = roc_curve(y_test, y_pred_probs)
        roc_auc = auc(fpr, tpr)
        
        fold_auc_scores.append(roc_auc)
        fold_fprs.append(fpr)
        fold_tprs.append(tpr)
    
    # STORING AVERAGE TEST AND TRAIN ACCURACIES
    average_train_acc[response_name] = np.mean(train_acc_scores)
    average_test_acc[response_name] = np.mean(test_acc_scores)
    auc_scores[response_name] = fold_auc_scores
    roc_curves[response_name] = (fold_fprs, fold_tprs)
```

## Results

### Bayes classifiers – Cameron

While the naive Bayes and QDA models predicted most of the outcome diagnoses with poor mean cross-validated accuracies (with the only models producing accuracies over 75% being the naive Bayes models for predicting asthma, congestive heart failure, and heart attack diagnoses), the LDA classification models predicted with consistently greater accuracy: all diagnosis outcomes except for arthritis were predicted with mean accuracies over 75% (@fig-accuracies). Initially, we considered this to suggest the LDA models were performing somewhat well.

![Mean 10-fold cross-validated accuracies for predicting each of the 8 diagnosis outcomes from the 43 nutrient intake predictors, separated by method of Bayes classification (naive Bayes, LDA, or QDA).](figures/bayes/bayes_classifiers_accuracy_comparison.png){#fig-accuracies}
Unfortunately, further investigation revealed that all of the models, including the LDAs, were in fact performing only slightly better than would a random classifier predicting solely on the basis of outcome value distribution (i.e., a classifier which, for a diagnosis outcome consisting of 95% "No/Not Diagnosed" values and only 5% "Yes/Diagnosed" values, simply classifies a random 95% of the observations as "No" without regard to predictor values). The expected receiver operator characteristic (ROC) curve for such a random classifier would be linear with a y-intercept of 0 and a slope of 1, and the ROC curves for our models are only marginally different from this (@fig-ROCs; with separated ROCs for the LDA models given in @fig-ROCs-LDA). The areas under the ROC curves (ROC-AUCs), which for a random classifier would have an expected value of 0.5, are also only marginally (though significantly, in the case of all of the LDA models) greater than 0.5 (@fig-ROC-AUCs).

![Receiver operator characteristic (ROC) curves for 10-fold cross-validated predictions of diagnosis outcomes from nutrient intake predictors, separated by method of Bayes classification (naive Bayes, LDA, or QDA).](figures/bayes/bayes_classifiers_ROC_curves_comparison.png){#fig-ROCs}

![Receiver operator characteristic (ROC) curves for 10-fold cross-validated LDA-classifier predictions of diagnosis outcomes from nutrient intake predictors, separated by diagnosis outcome.](figures/bayes/LDA_classifier_ROC_curves_comparison.png){#fig-ROCs-LDA}

![Areas under the receiver operator characteristic curves (ROC-AUCs) for 10-fold cross-validated predictions of diagnosis outcomes from nutrient intake predictors, separated by method of Bayes classification (naive Bayes, LDA, or QDA). The expected ROC-AUC of a random (uninformed by predictors) classifier, 0.5, is given by the dashed black line.](figures/bayes/bayes_classifiers_ROC-AUC_comparison.png){#fig-ROC-AUCs}

To determine if the models' poor performances were in-part due to the presence of outliers in the nutrient intake predictor variables, we removed observations from the data containing outliers (i.e., values less than their variable's 25th percentile$-1.5\mathrm{IQR}$ or greater than their variable's 75th percentile$+1.5\mathrm{IQR}$) in any of the predictors and re-trained the models with this subset of the data. This endeavor did not produce a substantial improvement in performance: while most of the models experienced a slight increase in ROC-AUC, the metric's values were still very near to 0.5 (@fig-ROC-AUCs-outliers) and the ROC curves themselves were largely unchanged in shape, besides exhibiting more variation due to the greater effect of noise on the smaller sample sizes (@fig-ROCs-outliers). 

![Areas under the receiver operator characteristic curves (ROC-AUCs) for 10-fold cross-validated predictions of diagnosis outcomes from nutrient intake predictors, using data from which observations with outliers in any predictors were removed, separated by method of Bayes classification (naive Bayes, LDA, or QDA). The expected ROC-AUC of a random (uninformed by predictors) classifier, 0.5, is given by the dashed black line.](figures/bayes/bayes_classifiers_ROC-AUC_comparison_no_outliers.png){#fig-ROC-AUCs-outliers}

![Receiver operator characteristic (ROC) curves for 10-fold cross-validated predictions of diagnosis outcomes from nutrient intake predictors, using data from which observations with outliers in any predictors were removed, separated by method of Bayes classification (naive Bayes, LDA, or QDA).](figures/bayes/bayes_classifiers_ROC_curves_comparison_no_outliers.png){#fig-ROCs-outliers}


Overall, then, we can reasonably conclude that these Bayes classifier models are incapable of reliably predicting diagnoses of any condition from nutrient intake in this dataset.

The code for creating the above plots is reproduced below:
```{r}
#| eval: false
#| file: 'bayes_classifiers.R'
#| echo: !expr 207:466
```

### Trees and random forest – Romy

indicate who did the work

```{r}

```

### Neural network – Alissa

```{python}
#| eval: false

# VERTICAL WHISKER PLOT FOR AUC SCORES
plt.figure(figsize=(12, 6))
sns.boxplot(data=pd.DataFrame(auc_scores), orient="v", palette="Set2")
plt.xticks(rotation=20, fontsize=8)
plt.ylabel("AUC Score")
plt.title("AUC Scores for Each Response Variable")
plt.show()

# ROC CURVES FOR EACH RESPONSE VARIABLE
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
fig.suptitle("ROC Curves for Each Response Variable", fontsize=14)

for ax, (response_name, (fprs, tprs)) in zip(axes.flatten(), roc_curves.items()):
    for fold in range(len(fprs)):
        ax.plot(fprs[fold], tprs[fold], lw=2, alpha=0.5, label=f"Fold {fold+1}")
    ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Random Guess")
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.set_title(response_name)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# CONFUSION MATRICES FOR ALL RESPONSE VARIABLES
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
fig.suptitle("Confusion Matrices for Each Response Variable", fontsize=14)

for ax, (response_name, conf_matrices) in zip(axes.flatten(), train_conf_matrices.items()):
    avg_conf_matrix = np.mean(conf_matrices, axis=0)
    avg_conf_matrix = np.round(avg_conf_matrix).astype(int)
    sns.heatmap(avg_conf_matrix, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"],
                ax=ax)
    ax.set_title(response_name)
    ax.set_xlabel("Predicted")
    ax.set_ylabel("True")

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# PREDICTION ACCURACY
plt.figure(figsize=(12, 6))
names = list(average_train_acc.keys())
train_acc_values = [average_train_acc[name] for name in names]
test_acc_values = [average_test_acc[name] for name in names]
x = range(len(names))
width = 0.4
plt.bar(x, train_acc_values, width=width, label='Train Accuracy', color='green', alpha=0.6)
plt.bar([p + width for p in x], test_acc_values, width=width, label='Test Accuracy', color='red', alpha=0.6)
plt.xticks([p + width/2 for p in x], names, rotation=20, fontsize=8)
plt.ylabel("Accuracy")
plt.title("Average Prediction Accuracy across 10-Fold CV for Each Response Variable")
plt.legend()
plt.yticks(fontsize=10, rotation=45)
plt.show()
```

**Neural Network Plots**

![](figures/NN/Full_data/ConfusionMatrices.png)

![](figures/NN/Full_data/AUC.png)

![](figures/NN/Full_data/AUC_curves.png)

![](figures/NN/Full_data/Accuracy.png)

**Neural Network with Undersampling Plots**

![](figures/NN/Undersampling/ConfusionMatrices.png)

![](figures/NN/Undersampling/AUC.png)

![](figures/NN/Undersampling/AUC_curves.png)

![](figures/NN/Undersampling/Accuracy.png)


## Conclusions

indicate who wrote it

Add a few paragraphs to describe what you learned and what new questions have been raised by your results.

## References

Links are best, but any format is fine as long as the reference is identifiable.
